{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini-Project1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicali1298/Applied_Machine_Learning/blob/master/Mini_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ8UbMS4GaFA",
        "colab_type": "text"
      },
      "source": [
        "###Task 1: Acquire, preprocess, and analyze the data\n",
        "1. Download the datasets (noting the correct subsets to use, as discussed above).\n",
        "2. Load the datasets into NumPy objects (i.e., arrays or matrices) in Python. Remember to convert the wine dataset\n",
        "to a binary task, as discussed above.\n",
        "3. Clean the data. Are there any missing or malformed features? Are there are other data oddities that need to be\n",
        "dealt with? You should remove any examples with missing or malformed features and note this in your\n",
        "report. For categorical variables you can use one-hot encoding.\n",
        "4. Compute basic statistics on the data to understand it better. E.g., what are the distributions of the positive vs.\n",
        "negative classes, what are the distributions of some of the numerical features? what are the correlations between\n",
        "the features? how does the scatter plots of pair-wise features look-like for some subset of features?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvjUTj8HKpp4",
        "colab_type": "text"
      },
      "source": [
        "###Task 2: Implement the models\n",
        "1. Implement logistic regression, and use (full batch) gradient descent for optimization.\n",
        "2. Implement naive Bayes, using the appropriate type of likelihood for features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUYqBLYwK2FU",
        "colab_type": "text"
      },
      "source": [
        "###Task 3: Run experiments\n",
        "1. Compare the accuracy of naive Bayes and logistic regression on the four datasets.\n",
        "2. Test different learning rates for gradient descent applied to logistic regression. Use a threshold for change in the\n",
        "value of the cost function as termination criteria, and plot the accuracy on train/validation set as a function of\n",
        "iterations of gradient descent.\n",
        "3. Compare the accuracy of the two models as a function of the size of dataset (by controlling the training size).\n",
        "As an example, see Figure 1 here 1"
      ]
    }
  ]
}